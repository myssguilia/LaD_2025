{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a83aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Test Batch ---\n",
      "Sentences to classify: 10\n",
      "Model: qwen3:1.7b\n",
      "--------------------------------------------------\n",
      "Model Output:\n",
      "Sentence: we just ↑remi[NISCE]  \n",
      "Label: STATEMENT  \n",
      "\n",
      "Sentence: [remi]nisce  \n",
      "Label: CLARIFICATION  \n",
      "\n",
      "Sentence: [eh]  \n",
      "Label: BACKCHANNEL  \n",
      "\n",
      "Sentence: (a)[bout] a shared memory  \n",
      "Label: REQUEST  \n",
      "\n",
      "Sentence: 'kay  \n",
      "Label: EXPRESSIVE  \n",
      "\n",
      "Sentence: m::h hhh ((tsk))  \n",
      "Label: OTHER  \n",
      "\n",
      "Sentence: which memory would you like  \n",
      "Label: QUESTION  \n",
      "\n",
      "Sentence: but i'm tryin' to think which memory is (0.6) within five minutes because they're all short (.) cute moments  \n",
      "Label: CLARIFICATION  \n",
      "\n",
      "Sentence: mh=yea:h  \n",
      "Label: EXPRESSIVE  \n",
      "\n",
      "Sentence: °we- we'll probably spend more than five minutes finding a memory°  \n",
      "Label: STATEMENT\n",
      "--------------------------------------------------\n",
      "Test Batch Complete. Total Time for 10 sentences: 72.91 seconds.\n",
      "Average time per sentence: 7.291 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL_NAME = \"qwen3:1.7b\"\n",
    "\n",
    "BATCH_SIZE = 10 \n",
    "FILE_PATH = \"solo_frasi.txt\"\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "\n",
    "BATCH_SYSTEM_PROMPT = f\"\"\"\n",
    "You are a highly efficient dialogue act classifier.\n",
    "\n",
    "Possible labels:\n",
    "STATEMENT, QUESTION, ANSWER, ACKNOWLEDGEMENT, BACKCHANNEL,\n",
    "DIRECTIVE, REQUEST, REPAIR, CLARIFICATION, EXPRESSIVE, EMOTIVE,\n",
    "APOLOGY, GREETING, GOODBYE, OTHER.\n",
    "\n",
    "RULES:\n",
    "- You MUST classify ALL TARGET utterances provided in the user prompt.\n",
    "- PREVIOUS is context, NOT classification target.\n",
    "- Ignore all transcription symbols (e.g., ((laugh)), °°, [ ], :, ↑, ↓).\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Sentence: <target_1>\n",
    "Label: <one_label_1>\n",
    "Sentence: <target_2>\n",
    "Label: <one_label_2>\n",
    "... (and so on for every target in the batch)\n",
    "\n",
    "Do NOT output any other text, markdown formatting, or comments.\n",
    "\"\"\"\n",
    "\n",
    "def create_batch_prompt(batch_data):\n",
    "    \"\"\"Creates a single, large user prompt for a batch of sentences.\"\"\"\n",
    "    user_prompt = \"Classify the following targets, using the preceding context:\\n\\n\"\n",
    "    \n",
    "    for i, (prev_u, target) in enumerate(batch_data):\n",
    "        user_prompt += f\"--- Target {i+1} ---\\n\"\n",
    "        user_prompt += f\"Previous: {prev_u}\\n\"\n",
    "        user_prompt += f\"Target: {target}\\n\"\n",
    "        \n",
    "    user_prompt += \"\\n--- END OF BATCH ---\\n\\nNow, provide the classification results using the specified OUTPUT FORMAT.\"\n",
    "    return user_prompt\n",
    "\n",
    "def annotate_batch(batch_data):\n",
    "    \"\"\"Invokes the model with a batch prompt and returns the raw response.\"\"\"\n",
    "    prompt = create_batch_prompt(batch_data)\n",
    "    \n",
    "    response = chat.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": BATCH_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.content.strip()\n",
    "\n",
    "def annotate_test_batch(path, batch_size):\n",
    "    \"\"\"\n",
    "    Loads data, prepares a single batch (up to batch_size), calls the model, \n",
    "    and prints the results and timing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = [l.strip() for l in f if l.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{path}' was not found.\")\n",
    "        return\n",
    "\n",
    "    # Prepare all (Previous, Target) pairs, limited to the batch size\n",
    "    all_data = []\n",
    "    for i, target in enumerate(lines[:batch_size]):\n",
    "        prev_u = lines[i-1] if i > 0 else \"\"\n",
    "        all_data.append((prev_u, target))\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No sentences found to annotate.\")\n",
    "        return\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    total_annotations = len(all_data)\n",
    "    \n",
    "    print(f\"--- Running Test Batch ---\")\n",
    "    print(f\"Sentences to classify: {total_annotations}\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get the batched classification result\n",
    "        batch_results_text = annotate_batch(all_data)\n",
    "        \n",
    "        # Print the results for this batch\n",
    "        print(\"Model Output:\")\n",
    "        print(batch_results_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"--- ERROR DURING INFERENCE ---\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    # End timer and print summary\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Test Batch Complete. Total Time for {total_annotations} sentences: {total_time:.2f} seconds.\")\n",
    "    if total_annotations > 0:\n",
    "        avg_time_per_sentence = total_time / total_annotations\n",
    "        print(f\"Average time per sentence: {avg_time_per_sentence:.3f} seconds.\")\n",
    "\n",
    "\n",
    "# Execute the test batch annotation\n",
    "annotate_test_batch(FILE_PATH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba528778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#https://ollama.com/models\n",
    "#ollama pull qwen3:1.7b\n",
    "\n",
    "dialogue_labels = ['STATEMENT', 'QUESTION', 'ANSWER', 'ACKNOWLEDGEMENT', 'BACKCHANNEL',\n",
    "'DIRECTIVE', 'REQUEST', 'REPAIR', 'CLARIFICATION', 'EXPRESSIVE', 'EMOTIVE',\n",
    "'APOLOGY', 'GREETING', 'GOODBYE', 'OTHER']\n",
    "\n",
    "examples = [\n",
    "    {'INPUT': 'we just ↑remi[NISCE]', 'OUTPUT': 'STATEMENT'},\n",
    "    {'INPUT': '((laughs))', 'OUTPUT': 'OTHER'},\n",
    "    {'INPUT': '[that- that is (.)] tr[ue]', 'OUTPUT': 'ACKNOWLEDGMENT'},\n",
    "    {'INPUT': '°yeah°', 'OUTPUT': 'BACKCHANNEL'},\n",
    "    {'INPUT': '°that was a good one°', 'OUTPUT': 'EXPRESSIVE'},\n",
    "]\n",
    "\n",
    "class LLMAnnotator:\n",
    "    \n",
    "    def __init__(self,  model=\"qwen3:1.7b\", labels=dialogue_labels, examples=examples, max_context=3):\n",
    "        \n",
    "        self._client = ChatOllama(\n",
    "            model=model,\n",
    "            think=False,\n",
    "            temperature= 0.0,\n",
    "            #num_predict = 20, ## max number of tokens to predict\n",
    "        )\n",
    "        self._max_context = max_context\n",
    "        self._history = []\n",
    "        self._instruct = []\n",
    "        self.create_label_instruct(labels, examples)\n",
    "\n",
    "    def create_label_instruct(self, labels =[], examples=[]):\n",
    "        self._instruct = [{\"role\": \"system\", \"content\": \"You are an expert annotator trained in Conversation Analysis and Dialogue Act tagging.\"}]\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"You must assign exactly ONE dialogue act label to EACH utterance.\"})\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"Utterances use JEFFERSON TRANSCRIPTION. Use them as cues for emotion, repair, overlap, etc., but do NOT treat them as words.\"})\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"Only use one of the following labels:{}.\".format(str(labels))}) #the label i'm giving to earlier function\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"Classify ONE UTTERANCE AT A TIME, use the preceding utterances as context.\"})\n",
    "        #self._instruct.append({\"role\": \"system\", \"content\": \"\")\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"Output the most appropriate label in JSON format.\"})\n",
    "        self._instruct.append({\"role\": \"system\", \"content\": \"Do not output anything else.\"})\n",
    "        if examples:\n",
    "            self._instruct.append({\"role\": \"system\", \"content\": \"Here are a few examples:\"})\n",
    "            for example in examples:\n",
    "                self._instruct.append({\"role\": \"user\", \"content\": example[\"INPUT\"]})\n",
    "                self._instruct.append({\"role\": \"system\", \"content\": example[\"OUTPUT\"]})\n",
    "        print(\"My instructions are:\", self._instruct)\n",
    "\n",
    "    def annotate_conversation(self, input=[]):\n",
    "        report = 5\n",
    "        annotations = []\n",
    "        counter = 0\n",
    "        start = datetime.now()\n",
    "        previous = start\n",
    "        print(\"Annotating a conversation with {} utterances\".format(len(input)))\n",
    "        for text in input:\n",
    "            annotation = self.annotate(text)\n",
    "            annotations.append(annotation)\n",
    "            counter+=1\n",
    "            if counter % report == 0:\n",
    "                now  = datetime.now()\n",
    "                print('Processed', report, 'in', (now - previous).seconds, 'seconds')\n",
    "                print(\"Processed\", counter, \"turns in total out of\", len(input))\n",
    "                previous = now\n",
    "        return annotations\n",
    "  \n",
    "    def annotate(self, utterance):\n",
    "        annotation ={}\n",
    "        self._history.append({\"role\": \"user\", \"content\": \"Input: {}\".format(utterance)})\n",
    "\n",
    "        ## if the history exceeds the maximum context length, we trim it by one\n",
    "        if len(self._history)>self._max_context:\n",
    "            self._history = self._history[1:]\n",
    "    \n",
    "        prompt = self._instruct+self._history\n",
    "        response = self._client.invoke(prompt)\n",
    "        \n",
    "        ### We need to remove the <think></think> part from the output\n",
    "        end_of_think = response.content.find(\"</think>\")\n",
    "        answer = response\n",
    "        if end_of_think>0:\n",
    "            think = response.content[:end_of_think+8]\n",
    "            answer = response.content[end_of_think+8:].replace(\"\\n\", \"\")\n",
    "        annotation={\"Input\": utterance, \"Output\": answer}\n",
    "        return annotation\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    ekman_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "#    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "#    input = [\"I am very happy\", \"I am surprised\", \"What can I do about it?\", \"Now I am having real fun.\", \"It make me sad and depressed\", \"Sorry to hear that\"]\n",
    "#    examples = [{\"Input\": \"I love dogs\", \"Output\": \"joy\"}, {\"Input\": \"I hate cats\", \"Output\": \"disgust\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a7c0446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('solo_frasi.txt', 'r') as infilez:\n",
    "    all_file = infilez.read()\n",
    "all_file = all_file.split('\\n')\n",
    "len(all_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "382d0afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My instructions are: [{'role': 'system', 'content': 'You are an expert annotator trained in Conversation Analysis and Dialogue Act tagging.'}, {'role': 'system', 'content': 'You must assign exactly ONE dialogue act label to EACH utterance.'}, {'role': 'system', 'content': 'Utterances use JEFFERSON TRANSCRIPTION. Use them as cues for emotion, repair, overlap, etc., but do NOT treat them as words.'}, {'role': 'system', 'content': \"Only use one of the following labels:['STATEMENT', 'QUESTION', 'ANSWER', 'ACKNOWLEDGEMENT', 'BACKCHANNEL', 'DIRECTIVE', 'REQUEST', 'REPAIR', 'CLARIFICATION', 'EXPRESSIVE', 'EMOTIVE', 'APOLOGY', 'GREETING', 'GOODBYE', 'OTHER'].\"}, {'role': 'system', 'content': 'Classify ONE UTTERANCE AT A TIME, use the preceding utterances as context.'}, {'role': 'system', 'content': 'Output the most appropriate label in JSON format.'}, {'role': 'system', 'content': 'Do not output anything else.'}, {'role': 'system', 'content': 'Here are a few examples:'}, {'role': 'user', 'content': 'we just ↑remi[NISCE]'}, {'role': 'system', 'content': 'STATEMENT'}, {'role': 'user', 'content': '((laughs))'}, {'role': 'system', 'content': 'OTHER'}, {'role': 'user', 'content': '[that- that is (.)] tr[ue]'}, {'role': 'system', 'content': 'ACKNOWLEDGMENT'}, {'role': 'user', 'content': '°yeah°'}, {'role': 'system', 'content': 'BACKCHANNEL'}, {'role': 'user', 'content': '°that was a good one°'}, {'role': 'system', 'content': 'EXPRESSIVE'}]\n",
      "Annotating a conversation with 10 utterances\n",
      "Processed 5 in 146 seconds\n",
      "Processed 5 turns in total out of 10\n",
      "Processed 5 in 142 seconds\n",
      "Processed 10 turns in total out of 10\n",
      "{'Input': 'we just ↑remi[NISCE]', 'Output': AIMessage(content='{\"utterance\": \"we just ↑remi[NISCE]\", \"label\": \"STATEMENT\"}, {\"utterance\": \"(laughs)\", \"label\": \"EMOTIVE\"}, {\"utterance\": \"[that- that is (.)] tr[ue]\", \"label\": \"STATEMENT\"}, {\"utterance\": \"yeah\", \"label\": \"STATEMENT\"}, {\"utterance\": \"that was a good one\", \"label\": \"STATEMENT\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:54:01.219479Z', 'done': True, 'done_reason': 'stop', 'total_duration': 26785784549, 'load_duration': 124509459, 'prompt_eval_count': 272, 'prompt_eval_duration': 38539351, 'eval_count': 524, 'eval_duration': 26332953091, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--11806551-8d0b-478f-8707-b6e86f781f13-0', usage_metadata={'input_tokens': 272, 'output_tokens': 524, 'total_tokens': 796})}\n",
      "{'Input': '[remi]nisce', 'Output': AIMessage(content='{\"label\": \"STATEMENT\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:54:15.995883Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14768889207, 'load_duration': 135312657, 'prompt_eval_count': 280, 'prompt_eval_duration': 433036973, 'eval_count': 251, 'eval_duration': 13975315192, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--aaf648b6-e08b-489d-a9c9-fe19172f1fec-0', usage_metadata={'input_tokens': 280, 'output_tokens': 251, 'total_tokens': 531})}\n",
      "{'Input': '[eh]', 'Output': AIMessage(content='{\"utterance\": \"we just ↑remi[NISCE]\", \"label\": \"STATEMENT\"}, {\"utterance\": \"[remi]nisce\", \"label\": \"STATEMENT\"}, {\"utterance\": \"[eh] /think\", \"label\": \"BACKCHANNEL\"}, {\"utterance\": \"°that was a good one°\", \"label\": \"EXPRESSIVE\"}, {\"utterance\": \"°yeah°\", \"label\": \"EXPRESSIVE\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:55:09.626485Z', 'done': True, 'done_reason': 'stop', 'total_duration': 53624352150, 'load_duration': 182473184, 'prompt_eval_count': 286, 'prompt_eval_duration': 341622079, 'eval_count': 918, 'eval_duration': 52609960909, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--ad6d4ae9-5a94-478d-80b2-851e200822e8-0', usage_metadata={'input_tokens': 286, 'output_tokens': 918, 'total_tokens': 1204})}\n",
      "{'Input': '(a)[bout] a shared memory', 'Output': AIMessage(content='{\"label\": \"STATEMENT\", \"utterance\": \"[remi]nisce\"}  \\n{\"label\": \"BACKCHANNEL\", \"utterance\": \"[eh]\"}  \\n{\"label\": \"QUESTION\", \"utterance\": \"(a)[bout] a shared memory\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:55:29.471106Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19840362274, 'load_duration': 128815529, 'prompt_eval_count': 285, 'prompt_eval_duration': 688540491, 'eval_count': 440, 'eval_duration': 18782348852, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--362232ac-e33c-4b56-a114-aae5a966641c-0', usage_metadata={'input_tokens': 285, 'output_tokens': 440, 'total_tokens': 725})}\n",
      "{'Input': \"'kay\", 'Output': AIMessage(content='{\"utterances\": [\"STATEMENT\", \"BACKCHANNEL\", \"ACKNOWLEDGEMENT\", \"ACKNOWLEDGEMENT\", \"EMOTIVE\", \"BACKCHANNEL\", \"STATEMENT\", \"BACKCHANNEL\"]}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:56:00.590433Z', 'done': True, 'done_reason': 'stop', 'total_duration': 31115259784, 'load_duration': 104474671, 'prompt_eval_count': 282, 'prompt_eval_duration': 493141156, 'eval_count': 660, 'eval_duration': 30144641288, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--d0b89307-07ac-403c-9386-c7970e36a275-0', usage_metadata={'input_tokens': 282, 'output_tokens': 660, 'total_tokens': 942})}\n",
      "{'Input': 'm::h hhh ((tsk))', 'Output': AIMessage(content='{\"0\": \"STATEMENT\", \"1\": \"EXPRESSIVE\", \"2\": \"BACKCHANNEL\", \"3\": \"STATEMENT\", \"4\": \"EXPRESSIVE\", \"5\": \"STATEMENT\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:56:38.363262Z', 'done': True, 'done_reason': 'stop', 'total_duration': 37769231614, 'load_duration': 118075836, 'prompt_eval_count': 289, 'prompt_eval_duration': 760909446, 'eval_count': 774, 'eval_duration': 36505924787, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--2f236c87-43cf-4614-a884-34022fc5018f-0', usage_metadata={'input_tokens': 289, 'output_tokens': 774, 'total_tokens': 1063})}\n",
      "{'Input': 'which memory would you like', 'Output': AIMessage(content='{\"label\": \"STATEMENT\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:56:55.151369Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16783128842, 'load_duration': 163580532, 'prompt_eval_count': 285, 'prompt_eval_duration': 637262757, 'eval_count': 314, 'eval_duration': 15788246655, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--fbaa02cc-5286-4676-a73e-63cf66a39624-0', usage_metadata={'input_tokens': 285, 'output_tokens': 314, 'total_tokens': 599})}\n",
      "{'Input': \"but i'm tryin' to think which memory is (0.6) within five minutes because they're all short (.) cute moments\", 'Output': AIMessage(content='{\\n  \"utterance 1\": \"EXPRESSIVE\",\\n  \"utterance 2\": \"QUESTION\",\\n  \"utterance 3\": \"STATEMENT\"\\n}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:57:30.49687Z', 'done': True, 'done_reason': 'stop', 'total_duration': 35342253391, 'load_duration': 123467481, 'prompt_eval_count': 310, 'prompt_eval_duration': 1206710568, 'eval_count': 679, 'eval_duration': 33647328357, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--dc63c703-3c32-497d-acdf-97747ffdf8bc-0', usage_metadata={'input_tokens': 310, 'output_tokens': 679, 'total_tokens': 989})}\n",
      "{'Input': 'mh=yea:h', 'Output': AIMessage(content=\"{'label': 'DIRECTIVE'}\", additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:57:44.491001Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13989842112, 'load_duration': 120699629, 'prompt_eval_count': 307, 'prompt_eval_duration': 1216703663, 'eval_count': 252, 'eval_duration': 12484567945, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--46966586-21a0-46b5-84df-a3ced3a04bdb-0', usage_metadata={'input_tokens': 307, 'output_tokens': 252, 'total_tokens': 559})}\n",
      "{'Input': \"°we- we'll probably spend more than five minutes finding a memory°\", 'Output': AIMessage(content='{\"1\": \"STATEMENT\", \"2\": \"BACKCHANNEL\", \"3\": \"STATEMENT\"}', additional_kwargs={}, response_metadata={'model': 'qwen3:1.7b', 'created_at': '2025-11-30T11:58:23.298955Z', 'done': True, 'done_reason': 'stop', 'total_duration': 38804087737, 'load_duration': 112099489, 'prompt_eval_count': 317, 'prompt_eval_duration': 1200043410, 'eval_count': 762, 'eval_duration': 37060007700, 'logprobs': None, 'model_name': 'qwen3:1.7b', 'model_provider': 'ollama'}, id='lc_run--11979aa9-4b68-4929-ae0c-c5ebf24e9208-0', usage_metadata={'input_tokens': 317, 'output_tokens': 762, 'total_tokens': 1079})}\n"
     ]
    }
   ],
   "source": [
    "llm_annotator  = LLMAnnotator(labels=dialogue_labels, examples=examples, max_context=3)\n",
    "annotations = llm_annotator.annotate_conversation(all_file[:10])\n",
    "for annotation in annotations:\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aba7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/10 in 160 seconds\n",
      "Processed 10/10 in 255 seconds\n",
      "{'Input': 'we just ↑remi[NISCE]', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': '[remi]nisce', 'Output': {'label': 'OTHER'}}\n",
      "{'Input': '[eh]', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': '(a)[bout] a shared memory', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': \"'kay\", 'Output': {'label': 'BACKCHANNEL'}}\n",
      "{'Input': 'm::h hhh ((tsk))', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': 'which memory would you like', 'Output': {'label': 'REQUEST'}}\n",
      "{'Input': \"but i'm tryin' to think which memory is (0.6) within five minutes because they're all short (.) cute moments\", 'Output': {'label': 'QUESTION'}}\n",
      "{'Input': 'mh=yea:h', 'Output': {'label': 'REQUEST'}}\n",
      "{'Input': \"°we- we'll probably spend more than five minutes finding a memory°\", 'Output': {'label': 'STATEMENT'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "dialogue_labels = ['STATEMENT', 'QUESTION', 'ANSWER', 'ACKNOWLEDGEMENT', 'BACKCHANNEL',\n",
    "'DIRECTIVE', 'REQUEST', 'REPAIR', 'CLARIFICATION', 'EXPRESSIVE', 'EMOTIVE',\n",
    "'APOLOGY', 'GREETING', 'GOODBYE', 'OTHER']\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {'INPUT': 'we just ↑remi[NISCE]', 'OUTPUT': 'STATEMENT'},\n",
    "    {'INPUT': '((laughs))', 'OUTPUT': 'OTHER'},\n",
    "    {'INPUT': '[that- that is (.)] tr[ue]', 'OUTPUT': 'ACKNOWLEDGEMENT'},\n",
    "    {'INPUT': '°yeah°', 'OUTPUT': 'BACKCHANNEL'},\n",
    "    {'INPUT': '°that was a good one°', 'OUTPUT': 'EXPRESSIVE'},\n",
    "]\n",
    "\n",
    "\n",
    "class LLMAnnotator:\n",
    "    \n",
    "    def __init__(self, model=\"qwen3:1.7b\", labels=dialogue_labels, examples=examples, max_context=3):\n",
    "        self._client = ChatOllama(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        self._max_context = max_context\n",
    "        self._history = []\n",
    "        self._instruct = self._build_instructions(labels, examples)\n",
    "\n",
    "    def _build_instructions(self, labels, examples):\n",
    "        instruct = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an expert annotator trained in Conversation Analysis and Dialogue Act tagging.\\n\"\n",
    "                \"You must output EXACTLY ONE label for the current utterance.\\n\"\n",
    "                \"Allowed labels: \" + \", \".join(labels) + \"\\n\"\n",
    "                \"Output MUST be a JSON object of the form: {\\\"label\\\": \\\"...\\\"}\\n\"\n",
    "                \"Do NOT output multiple labels. Do NOT output multiple JSON objects. Do NOT explain.\\n\"\n",
    "                \"Only classify the *current* input.\\n\"\n",
    "            )}\n",
    "        ]\n",
    "\n",
    "        # few-shot examples: proper format (user -> assistant)\n",
    "        for ex in examples:\n",
    "            instruct.append({\"role\": \"user\", \"content\": ex[\"INPUT\"]})\n",
    "            instruct.append({\"role\": \"assistant\", \"content\": json.dumps({\"label\": ex[\"OUTPUT\"]})})\n",
    "\n",
    "        return instruct\n",
    "\n",
    "    def _clean_response(self, text):\n",
    "        # remove <think> blocks if present\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "        # extract JSON object\n",
    "        m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "        if not m:\n",
    "            return {\"label\": \"OTHER\"}\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except:\n",
    "            # fallback: extract label manually\n",
    "            for lbl in dialogue_labels:\n",
    "                if lbl.lower() in text.lower():\n",
    "                    return {\"label\": lbl}\n",
    "        return {\"label\": \"OTHER\"}\n",
    "\n",
    "    def annotate(self, utterance):\n",
    "        # Update short sliding context\n",
    "        self._history.append({\"role\": \"user\", \"content\": utterance})\n",
    "        if len(self._history) > self._max_context:\n",
    "            self._history = self._history[-self._max_context:]\n",
    "\n",
    "        prompt = self._instruct + self._history\n",
    "        response = self._client.invoke(prompt)\n",
    "        cleaned = self._clean_response(response.content)\n",
    "\n",
    "        return {\"Input\": utterance, \"Output\": cleaned}\n",
    "\n",
    "    def annotate_conversation(self, utterances):\n",
    "        annotations = []\n",
    "        start = datetime.now()\n",
    "        for idx, utt in enumerate(utterances, 1):\n",
    "            ann = self.annotate(utt)\n",
    "            annotations.append(ann)\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx}/{len(utterances)} in {(datetime.now() - start).seconds} seconds\")\n",
    "        return annotations\n",
    "\n",
    "\n",
    "\n",
    "# --- RUNNING ---\n",
    "with open('solo_frasi.txt', 'r') as infile:\n",
    "    lines = infile.read().split(\"\\n\")\n",
    "\n",
    "llm = LLMAnnotator(labels=dialogue_labels, examples=examples)\n",
    "annotations = llm.annotate_conversation(lines[:10])\n",
    "\n",
    "for ann in annotations:\n",
    "    print(ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e08eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/10 in 162 seconds\n",
      "Processed 10/10 in 323 seconds\n",
      "{'Input': 'we just ↑remi[NISCE]', 'Output': {'label': 'DIRECTIVE'}}\n",
      "{'Input': '[remi]nisce', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': '[eh]', 'Output': {'label': 'BACKCHANNEL'}}\n",
      "{'Input': '(a)[bout] a shared memory', 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': \"'kay\", 'Output': {'label': 'STATEMENT'}}\n",
      "{'Input': 'm::h hhh ((tsk))', 'Output': {'label': 'ANSWER'}}\n",
      "{'Input': 'which memory would you like', 'Output': {'label': 'OTHER'}}\n",
      "{'Input': \"but i'm tryin' to think which memory is (0.6) within five minutes because they're all short (.) cute moments\", 'Output': {'label': 'REQUEST'}}\n",
      "{'Input': 'mh=yea:h', 'Output': {'label': 'QUESTION'}}\n",
      "{'Input': \"°we- we'll probably spend more than five minutes finding a memory°\", 'Output': {'label': 'STATEMENT'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dialogue_labels = [\n",
    "    'STATEMENT', 'QUESTION', 'ANSWER', 'ACKNOWLEDGEMENT', 'BACKCHANNEL',\n",
    "    'DIRECTIVE', 'REQUEST', 'REPAIR', 'CLARIFICATION', 'EXPRESSIVE',\n",
    "    'EMOTIVE', 'APOLOGY', 'GREETING', 'GOODBYE', 'OTHER'\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {'INPUT': 'we just ↑remi[NISCE]', 'OUTPUT': 'STATEMENT'},\n",
    "    {'INPUT': '((laughs))', 'OUTPUT': 'OTHER'},\n",
    "    {'INPUT': '[that- that is (.)] tr[ue]', 'OUTPUT': 'ACKNOWLEDGEMENT'},\n",
    "    {'INPUT': '°yeah°', 'OUTPUT': 'BACKCHANNEL'},\n",
    "    {'INPUT': '°that was a good one°', 'OUTPUT': 'EXPRESSIVE'},\n",
    "]\n",
    "\n",
    "\n",
    "class LLMAnnotator:\n",
    "    \n",
    "    def __init__(self, model=\"qwen3:1.7b\", labels=dialogue_labels, examples=examples, max_context=3):\n",
    "        self._client = ChatOllama(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        self._max_context = max_context\n",
    "        self._history = []\n",
    "        self._instruct = self._build_instructions(labels, examples)\n",
    "\n",
    "    def _build_instructions(self, labels, examples):\n",
    "        instruct = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert annotator of dialogue acts.\\n\"\n",
    "                    \"You MUST output exactly one label for the current utterance.\\n\"\n",
    "                    \"Allowed labels: \" + \", \".join(labels) + \"\\n\"\n",
    "                    \"Output must be a JSON object: {\\\"label\\\": \\\"...\\\"}\\n\"\n",
    "                    \"Do NOT output anything else.\\n\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Correct few-shot examples (user → assistant)\n",
    "        for ex in examples:\n",
    "            instruct.append({\"role\": \"user\", \"content\": ex[\"INPUT\"]})\n",
    "            instruct.append({\"role\": \"assistant\", \"content\": json.dumps({\"label\": ex[\"OUTPUT\"]})})\n",
    "\n",
    "        return instruct\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        # Strip possible <think> blocks\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "        # Extract only the first JSON object\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "\n",
    "        if start != -1 and end != -1:\n",
    "            candidate = text[start:end+1]\n",
    "            try:\n",
    "                parsed = json.loads(candidate)\n",
    "                return parsed\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # fallback if malformed\n",
    "        for lbl in dialogue_labels:\n",
    "            if lbl.lower() in text.lower():\n",
    "                return {\"label\": lbl}\n",
    "\n",
    "        return {\"label\": \"OTHER\"}\n",
    "\n",
    "    def annotate(self, utterance):\n",
    "        # Add utterance to context WITHOUT the \"Input:\" prefix\n",
    "        self._history.append({\"role\": \"user\", \"content\": utterance})\n",
    "\n",
    "        # Trim context\n",
    "        if len(self._history) > self._max_context:\n",
    "            self._history = self._history[-self._max_context:]\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = self._instruct + self._history\n",
    "\n",
    "        # Query model\n",
    "        response = self._client.invoke(prompt)\n",
    "        cleaned = self._extract_json(response.content)\n",
    "\n",
    "        return {\"Input\": utterance, \"Output\": cleaned}\n",
    "\n",
    "    def annotate_conversation(self, utterances):\n",
    "        annotations = []\n",
    "        start = datetime.now()\n",
    "\n",
    "        for idx, utt in enumerate(utterances, 1):\n",
    "            ann = self.annotate(utt)\n",
    "            annotations.append(ann)\n",
    "\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx}/{len(utterances)} in {(datetime.now() - start).seconds} seconds\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# ============================\n",
    "# RUN THE ANNOTATOR\n",
    "# ============================\n",
    "\n",
    "with open('solo_frasi.txt', 'r') as infile:\n",
    "    lines = infile.read().split(\"\\n\")\n",
    "\n",
    "llm = LLMAnnotator(labels=dialogue_labels, examples=examples, max_context=3)\n",
    "annotations = llm.annotate_conversation(lines[:10])\n",
    "\n",
    "for ann in annotations:\n",
    "    print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46418b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dialogue_labels = [\n",
    "    'STATEMENT', 'QUESTION', 'ANSWER', 'ACKNOWLEDGEMENT', 'BACKCHANNEL',\n",
    "    'DIRECTIVE', 'REQUEST', 'REPAIR', 'CLARIFICATION', 'EXPRESSIVE',\n",
    "    'EMOTIVE', 'APOLOGY', 'GREETING', 'GOODBYE', 'OTHER'\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {'INPUT': 'we just ↑remi[NISCE]', 'OUTPUT': 'STATEMENT'},\n",
    "    {'INPUT': '((laughs))', 'OUTPUT': 'OTHER'},\n",
    "    {'INPUT': '[that- that is (.)] tr[ue]', 'OUTPUT': 'ACKNOWLEDGEMENT'},\n",
    "    {'INPUT': '°yeah°', 'OUTPUT': 'BACKCHANNEL'},\n",
    "    {'INPUT': '°that was a good one°', 'OUTPUT': 'EXPRESSIVE'},\n",
    "]\n",
    "\n",
    "\n",
    "class LLMAnnotator:\n",
    "    \n",
    "    def __init__(self, model=\"qwen3:1.7b\", labels=dialogue_labels, examples=examples, max_context=3):\n",
    "        self._client = ChatOllama(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        self._max_context = max_context\n",
    "        self._history = []\n",
    "        self._instruct = self._build_instructions(labels, examples)\n",
    "\n",
    "    def _build_instructions(self, labels, examples):\n",
    "        instruct = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert annotator of dialogue acts.\\n\"\n",
    "                    \"You MUST output exactly one label for the current utterance.\\n\"\n",
    "                    \"Allowed labels: \" + \", \".join(labels) + \"\\n\"\n",
    "                    \"Output must be a JSON object: {\\\"label\\\": \\\"...\\\"}\\n\"\n",
    "                    \"Do NOT output anything else.\\n\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Correct few-shot examples (user → assistant)\n",
    "        for ex in examples:\n",
    "            instruct.append({\"role\": \"user\", \"content\": ex[\"INPUT\"]})\n",
    "            instruct.append({\"role\": \"assistant\", \"content\": json.dumps({\"label\": ex[\"OUTPUT\"]})})\n",
    "\n",
    "        return instruct\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        # Strip possible <think> blocks\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "        # Extract only the first JSON object\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "\n",
    "        if start != -1 and end != -1:\n",
    "            candidate = text[start:end+1]\n",
    "            try:\n",
    "                parsed = json.loads(candidate)\n",
    "                return parsed\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # fallback if malformed\n",
    "        for lbl in dialogue_labels:\n",
    "            if lbl.lower() in text.lower():\n",
    "                return {\"label\": lbl}\n",
    "\n",
    "        return {\"label\": \"OTHER\"}\n",
    "\n",
    "    def annotate(self, utterance):\n",
    "        # Add utterance to context WITHOUT the \"Input:\" prefix\n",
    "        self._history.append({\"role\": \"user\", \"content\": utterance})\n",
    "\n",
    "        # Trim context\n",
    "        if len(self._history) > self._max_context:\n",
    "            self._history = self._history[-self._max_context:]\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = self._instruct + self._history\n",
    "\n",
    "        # Query model\n",
    "        response = self._client.invoke(prompt)\n",
    "        cleaned = self._extract_json(response.content)\n",
    "\n",
    "        return {\"Input\": utterance, \"Output\": cleaned}\n",
    "\n",
    "    def annotate_conversation(self, utterances):\n",
    "        annotations = []\n",
    "        start = datetime.now()\n",
    "\n",
    "        for idx, utt in enumerate(utterances, 1):\n",
    "            ann = self.annotate(utt)\n",
    "            annotations.append(ann)\n",
    "\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx}/{len(utterances)} in {(datetime.now() - start).seconds} seconds\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# ============================\n",
    "# RUN THE ANNOTATOR\n",
    "# ============================\n",
    "\n",
    "with open('solo_frasi.txt', 'r') as infile:\n",
    "    lines = infile.read().split(\"\\n\")\n",
    "\n",
    "llm = LLMAnnotator(labels=dialogue_labels, examples=examples, max_context=3)\n",
    "annotations = llm.annotate_conversation(lines[:10])\n",
    "\n",
    "for ann in annotations:\n",
    "    print(ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddc8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
